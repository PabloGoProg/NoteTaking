Tokens, in the context of AI, are ==little peaces of text== - These can be words, a peace of a word, chars combinations and even tildes.

If images or audio would be included, tokens could be patches of that image or chucks of that sound.

**Ex**: Hello World! -> Hello - World - ! (3 Tokens)
### Tokenization
Convert a long text into a _token_ array - Every [[LLMs]] has its own tokenization model. (Ex: GPT uses BPE (Byte-Pair-Encoding))
### Token Constraints
- Each model has a limit of inputs token it can consume (4097 token for both, input and output in GPT).





